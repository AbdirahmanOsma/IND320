{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "96f83ac2",
   "metadata": {},
   "source": [
    "# IND320 – Part 4  \n",
    "## Data Retrieval and Storage (Production & Consumption 2021–2024)\n",
    "\n",
    "This notebook retrieves hourly energy production and consumption data from the Elhub API, using the same approach as in Part 2.  \n",
    "The new data (2022–2024) are appended to the 2021 data and stored in both Cassandra and MongoDB.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc3be8a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:cassandra.cluster:Cluster.__init__ called with contact_points specified, but no load_balancing_policy. In the next major version, this will raise an error; please specify a load-balancing policy. (contact_points = ['127.0.0.1'], lbp = None)\n",
      "WARNING:cassandra.cluster:Downgrading core protocol version from 66 to 65 for 127.0.0.1:9042. To avoid this, it is best practice to explicitly set Cluster(protocol_version) to the version supported by your cluster. http://datastax.github.io/python-driver/api/cassandra/cluster.html#cassandra.cluster.Cluster.protocol_version\n",
      "WARNING:cassandra.cluster:Downgrading core protocol version from 65 to 5 for 127.0.0.1:9042. To avoid this, it is best practice to explicitly set Cluster(protocol_version) to the version supported by your cluster. http://datastax.github.io/python-driver/api/cassandra/cluster.html#cassandra.cluster.Cluster.protocol_version\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Cassandra connected successfully\n"
     ]
    }
   ],
   "source": [
    "from cassandra.cluster import Cluster\n",
    "\n",
    "try:\n",
    "    cluster = Cluster([\"127.0.0.1\"], port=9042)\n",
    "    session = cluster.connect()\n",
    "    print(\"✅ Cassandra connected successfully\")\n",
    "except Exception as e:\n",
    "        print(\"❌ Failed to connect to Cassandra:\", repr(e))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a04ccba4",
   "metadata": {},
   "source": [
    "Creating keyspace and tabeles for Part4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "fa785a02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Production table ready\n",
      "Consumption table ready\n"
     ]
    }
   ],
   "source": [
    "KEYSPACE = \"my_keyspace\"\n",
    "\n",
    "# Change keyspace first\n",
    "session.execute(f\"USE {KEYSPACE};\")\n",
    "\n",
    "# New table names for part 4\n",
    "PROD_TABLE = \"production_per_group_hour_2022_2024\"\n",
    "CONS_TABLE = \"consumption_per_group_hour_2021_2024\"\n",
    "\n",
    "# --- Production table 2022–2024 ---\n",
    "session.execute(f\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS {PROD_TABLE} (\n",
    "    pricearea text,\n",
    "    productiongroup text,\n",
    "    starttime timestamp,\n",
    "    quantitykwh double,\n",
    "    PRIMARY KEY ((pricearea, productiongroup), starttime)\n",
    ") WITH CLUSTERING ORDER BY (starttime ASC);\n",
    "\"\"\")\n",
    "\n",
    "print(\"Production table ready\")\n",
    "\n",
    "# --- Consumption table 2021–2024 ---\n",
    "session.execute(f\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS {CONS_TABLE} (\n",
    "    pricearea text,\n",
    "    consumptiongroup text,\n",
    "    starttime timestamp,\n",
    "    quantitykwh double,\n",
    "    PRIMARY KEY ((pricearea, consumptiongroup), starttime)\n",
    ") WITH CLUSTERING ORDER BY (starttime ASC);\n",
    "\"\"\")\n",
    "\n",
    "print(\"Consumption table ready\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94b82e31",
   "metadata": {},
   "source": [
    "API Helper Function\n",
    "\n",
    "## Below API helper functions\n",
    "Below I import and reuse the helper functions from Part 2, but I generalize them so they can fetch\n",
    "multiple years and price areas. These helpers will be used to download hourly data month-by-month."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b8a45c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Elhub API helper functions (reuse from Part 2, slightly generalized) ---\n",
    "import requests\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from dateutil.relativedelta import relativedelta\n",
    "\n",
    "BASE_URL = \"https://api.elhub.no/energy-data/v0/price-areas\"\n",
    "TIMEZONE_OFFSET = \"%2B02:00\"   # +02:00 URL encoded\n",
    "PRICE_AREAS = [\"NO1\", \"NO2\", \"NO3\", \"NO4\", \"NO5\"]\n",
    "\n",
    "\n",
    "def month_edges(year: int):\n",
    "    \"\"\"Generate (start, end) pairs for each month in a year.\"\"\"\n",
    "    cur = datetime(year, 1, 1)\n",
    "    end = datetime(year + 1, 1, 1)\n",
    "    while cur < end:\n",
    "        nxt = cur + relativedelta(months=1)\n",
    "        yield cur, min(nxt, end)\n",
    "        cur = nxt\n",
    "\n",
    "\n",
    "def fetch_month(start_dt: datetime, end_dt: datetime, area: str, dataset: str):\n",
    "    \"\"\"\n",
    "    Fetch a single month for one price area and one dataset.\n",
    "    dataset is e.g. 'PRODUCTION_PER_GROUP_MBA_HOUR' or 'CONSUMPTION_PER_GROUP_MBA_HOUR'\n",
    "    \"\"\"\n",
    "    start_str = start_dt.strftime(\"%Y-%m-%dT%H:%M:%S\") + TIMEZONE_OFFSET\n",
    "    end_str   = end_dt.strftime(\"%Y-%m-%dT%H:%M:%S\")   + TIMEZONE_OFFSET\n",
    "\n",
    "    url = (\n",
    "        f\"{BASE_URL}?dataset={dataset}\"\n",
    "        f\"&startDate={start_str}&endDate={end_str}\"\n",
    "        f\"&priceArea={area}\"\n",
    "    )\n",
    "    r = requests.get(url, timeout=60)\n",
    "    r.raise_for_status()\n",
    "    js = r.json()\n",
    "\n",
    "    # Decide which key to look for in the JSON\n",
    "    if \"PRODUCTION\" in dataset.upper():\n",
    "        key_name = \"productionPerGroupMbaHour\"\n",
    "    else:\n",
    "        key_name = \"consumptionPerGroupMbaHour\"\n",
    "\n",
    "    # Form 1: top-level list\n",
    "    if isinstance(js.get(key_name), list):\n",
    "        return js[key_name]\n",
    "\n",
    "    # Form 2: { \"data\": [ { \"attributes\": { key_name: [...] }}, ... ] }\n",
    "    rows = []\n",
    "    for entity in js.get(\"data\", []):\n",
    "        attrs = (entity or {}).get(\"attributes\", {}) or {}\n",
    "        rows.extend(attrs.get(key_name, []))\n",
    "    return rows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a319cbd2",
   "metadata": {},
   "source": [
    "## Fetching and cleaning Elhub data\n",
    "\n",
    "Here I download hourly production (2022–2024) and consumption (2021–2024) data\n",
    "from the Elhub API, looping month-by-month for all price areas (NO1–NO5).\n",
    "The `fetch_month()` function handles both possible JSON response formats.\n",
    "\n",
    "After downloading, I convert the raw data into DataFrames, rename columns to match\n",
    "the Cassandra schema, convert timestamps, ensure numeric types, remove missing\n",
    "values, and drop duplicates. The cleaned datasets (`df_prod_norm` and `df_cons_norm`)\n",
    "are then ready to be inserted into Cassandra."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "347fbefd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROD 2022 Jan NO1: 18600 rows\n",
      "PROD 2022 Feb NO1: 16800 rows\n",
      "PROD 2022 Mar NO1: 18575 rows\n",
      "PROD 2022 Apr NO1: 18000 rows\n",
      "PROD 2022 May NO1: 18600 rows\n",
      "PROD 2022 Jun NO1: 18000 rows\n",
      "PROD 2022 Jul NO1: 18600 rows\n",
      "PROD 2022 Aug NO1: 18600 rows\n",
      "PROD 2022 Sep NO1: 18000 rows\n",
      "PROD 2022 Oct NO1: 18625 rows\n",
      "PROD 2022 Nov NO1: 18000 rows\n",
      "PROD 2022 Dec NO1: 18600 rows\n",
      "PROD 2022 Jan NO2: 18600 rows\n",
      "PROD 2022 Feb NO2: 16800 rows\n",
      "PROD 2022 Mar NO2: 18575 rows\n",
      "PROD 2022 Apr NO2: 18000 rows\n",
      "PROD 2022 May NO2: 18600 rows\n",
      "PROD 2022 Jun NO2: 18000 rows\n",
      "PROD 2022 Jul NO2: 18600 rows\n",
      "PROD 2022 Aug NO2: 18600 rows\n",
      "PROD 2022 Sep NO2: 18000 rows\n",
      "PROD 2022 Oct NO2: 18625 rows\n",
      "PROD 2022 Nov NO2: 18000 rows\n",
      "PROD 2022 Dec NO2: 18600 rows\n",
      "PROD 2022 Jan NO3: 18600 rows\n",
      "PROD 2022 Feb NO3: 16800 rows\n",
      "PROD 2022 Mar NO3: 18575 rows\n",
      "PROD 2022 Apr NO3: 18000 rows\n",
      "PROD 2022 May NO3: 18600 rows\n",
      "PROD 2022 Jun NO3: 18000 rows\n",
      "PROD 2022 Jul NO3: 18600 rows\n",
      "PROD 2022 Aug NO3: 18600 rows\n",
      "PROD 2022 Sep NO3: 18000 rows\n",
      "PROD 2022 Oct NO3: 18625 rows\n",
      "PROD 2022 Nov NO3: 18000 rows\n",
      "PROD 2022 Dec NO3: 18600 rows\n",
      "PROD 2022 Jan NO4: 18600 rows\n",
      "PROD 2022 Feb NO4: 16800 rows\n",
      "PROD 2022 Mar NO4: 18575 rows\n",
      "PROD 2022 Apr NO4: 18000 rows\n",
      "PROD 2022 May NO4: 18600 rows\n",
      "PROD 2022 Jun NO4: 18000 rows\n",
      "PROD 2022 Jul NO4: 18600 rows\n",
      "PROD 2022 Aug NO4: 18600 rows\n",
      "PROD 2022 Sep NO4: 18000 rows\n",
      "PROD 2022 Oct NO4: 18625 rows\n",
      "PROD 2022 Nov NO4: 18000 rows\n",
      "PROD 2022 Dec NO4: 18600 rows\n",
      "PROD 2022 Jan NO5: 18600 rows\n",
      "PROD 2022 Feb NO5: 16800 rows\n",
      "PROD 2022 Mar NO5: 18575 rows\n",
      "PROD 2022 Apr NO5: 18000 rows\n",
      "PROD 2022 May NO5: 18600 rows\n",
      "PROD 2022 Jun NO5: 18000 rows\n",
      "PROD 2022 Jul NO5: 18600 rows\n",
      "PROD 2022 Aug NO5: 18600 rows\n",
      "PROD 2022 Sep NO5: 18000 rows\n",
      "PROD 2022 Oct NO5: 18625 rows\n",
      "PROD 2022 Nov NO5: 18000 rows\n",
      "PROD 2022 Dec NO5: 18600 rows\n",
      "PROD 2023 Jan NO1: 18600 rows\n",
      "PROD 2023 Feb NO1: 16800 rows\n",
      "PROD 2023 Mar NO1: 18575 rows\n",
      "PROD 2023 Apr NO1: 18000 rows\n",
      "PROD 2023 May NO1: 18600 rows\n",
      "PROD 2023 Jun NO1: 18000 rows\n",
      "PROD 2023 Jul NO1: 18600 rows\n",
      "PROD 2023 Aug NO1: 18600 rows\n",
      "PROD 2023 Sep NO1: 18000 rows\n",
      "PROD 2023 Oct NO1: 18625 rows\n",
      "PROD 2023 Nov NO1: 18000 rows\n",
      "PROD 2023 Dec NO1: 18600 rows\n",
      "PROD 2023 Jan NO2: 18600 rows\n",
      "PROD 2023 Feb NO2: 16800 rows\n",
      "PROD 2023 Mar NO2: 18575 rows\n",
      "PROD 2023 Apr NO2: 18000 rows\n",
      "PROD 2023 May NO2: 18600 rows\n",
      "PROD 2023 Jun NO2: 18000 rows\n",
      "PROD 2023 Jul NO2: 18600 rows\n",
      "PROD 2023 Aug NO2: 18600 rows\n",
      "PROD 2023 Sep NO2: 18000 rows\n",
      "PROD 2023 Oct NO2: 18625 rows\n",
      "PROD 2023 Nov NO2: 18000 rows\n",
      "PROD 2023 Dec NO2: 18600 rows\n",
      "PROD 2023 Jan NO3: 18600 rows\n",
      "PROD 2023 Feb NO3: 16800 rows\n",
      "PROD 2023 Mar NO3: 18575 rows\n",
      "PROD 2023 Apr NO3: 18000 rows\n",
      "PROD 2023 May NO3: 18600 rows\n",
      "PROD 2023 Jun NO3: 18000 rows\n",
      "PROD 2023 Jul NO3: 18600 rows\n",
      "PROD 2023 Aug NO3: 18600 rows\n",
      "PROD 2023 Sep NO3: 18000 rows\n",
      "PROD 2023 Oct NO3: 18625 rows\n",
      "PROD 2023 Nov NO3: 18000 rows\n",
      "PROD 2023 Dec NO3: 18600 rows\n",
      "PROD 2023 Jan NO4: 18600 rows\n",
      "PROD 2023 Feb NO4: 16800 rows\n",
      "PROD 2023 Mar NO4: 18575 rows\n",
      "PROD 2023 Apr NO4: 18000 rows\n",
      "PROD 2023 May NO4: 18600 rows\n",
      "PROD 2023 Jun NO4: 18000 rows\n",
      "PROD 2023 Jul NO4: 18600 rows\n",
      "PROD 2023 Aug NO4: 18600 rows\n",
      "PROD 2023 Sep NO4: 18000 rows\n",
      "PROD 2023 Oct NO4: 18625 rows\n",
      "PROD 2023 Nov NO4: 18000 rows\n",
      "PROD 2023 Dec NO4: 18600 rows\n",
      "PROD 2023 Jan NO5: 18600 rows\n",
      "PROD 2023 Feb NO5: 16800 rows\n",
      "PROD 2023 Mar NO5: 18575 rows\n",
      "PROD 2023 Apr NO5: 18000 rows\n",
      "PROD 2023 May NO5: 18600 rows\n",
      "PROD 2023 Jun NO5: 18000 rows\n",
      "PROD 2023 Jul NO5: 18600 rows\n",
      "PROD 2023 Aug NO5: 18600 rows\n",
      "PROD 2023 Sep NO5: 18000 rows\n",
      "PROD 2023 Oct NO5: 18625 rows\n",
      "PROD 2023 Nov NO5: 18000 rows\n",
      "PROD 2023 Dec NO5: 18600 rows\n",
      "PROD 2024 Jan NO1: 18600 rows\n",
      "PROD 2024 Feb NO1: 17400 rows\n",
      "PROD 2024 Mar NO1: 18575 rows\n",
      "PROD 2024 Apr NO1: 18000 rows\n",
      "PROD 2024 May NO1: 18600 rows\n",
      "PROD 2024 Jun NO1: 18000 rows\n",
      "PROD 2024 Jul NO1: 18600 rows\n",
      "PROD 2024 Aug NO1: 18600 rows\n",
      "PROD 2024 Sep NO1: 18000 rows\n",
      "PROD 2024 Oct NO1: 18625 rows\n",
      "PROD 2024 Nov NO1: 18000 rows\n",
      "PROD 2024 Dec NO1: 18600 rows\n",
      "PROD 2024 Jan NO2: 18600 rows\n",
      "PROD 2024 Feb NO2: 17400 rows\n",
      "PROD 2024 Mar NO2: 18575 rows\n",
      "PROD 2024 Apr NO2: 18000 rows\n",
      "PROD 2024 May NO2: 18600 rows\n",
      "PROD 2024 Jun NO2: 18000 rows\n",
      "PROD 2024 Jul NO2: 18600 rows\n",
      "PROD 2024 Aug NO2: 18600 rows\n",
      "PROD 2024 Sep NO2: 18000 rows\n",
      "PROD 2024 Oct NO2: 18625 rows\n",
      "PROD 2024 Nov NO2: 18000 rows\n",
      "PROD 2024 Dec NO2: 18600 rows\n",
      "PROD 2024 Jan NO3: 18600 rows\n",
      "PROD 2024 Feb NO3: 17400 rows\n",
      "PROD 2024 Mar NO3: 18575 rows\n",
      "PROD 2024 Apr NO3: 18000 rows\n",
      "PROD 2024 May NO3: 18600 rows\n",
      "PROD 2024 Jun NO3: 18000 rows\n",
      "PROD 2024 Jul NO3: 18600 rows\n",
      "PROD 2024 Aug NO3: 18600 rows\n",
      "PROD 2024 Sep NO3: 18000 rows\n",
      "PROD 2024 Oct NO3: 18625 rows\n",
      "PROD 2024 Nov NO3: 18000 rows\n",
      "PROD 2024 Dec NO3: 18600 rows\n",
      "PROD 2024 Jan NO4: 18600 rows\n",
      "PROD 2024 Feb NO4: 17400 rows\n",
      "PROD 2024 Mar NO4: 18575 rows\n",
      "PROD 2024 Apr NO4: 18000 rows\n",
      "PROD 2024 May NO4: 18600 rows\n",
      "PROD 2024 Jun NO4: 18000 rows\n",
      "PROD 2024 Jul NO4: 18600 rows\n",
      "PROD 2024 Aug NO4: 18600 rows\n",
      "PROD 2024 Sep NO4: 18000 rows\n",
      "PROD 2024 Oct NO4: 18625 rows\n",
      "PROD 2024 Nov NO4: 18000 rows\n",
      "PROD 2024 Dec NO4: 18600 rows\n",
      "PROD 2024 Jan NO5: 18600 rows\n",
      "PROD 2024 Feb NO5: 17400 rows\n",
      "PROD 2024 Mar NO5: 18575 rows\n",
      "PROD 2024 Apr NO5: 18000 rows\n",
      "PROD 2024 May NO5: 18600 rows\n",
      "PROD 2024 Jun NO5: 18000 rows\n",
      "PROD 2024 Jul NO5: 18600 rows\n",
      "PROD 2024 Aug NO5: 18600 rows\n",
      "PROD 2024 Sep NO5: 18000 rows\n",
      "PROD 2024 Oct NO5: 18625 rows\n",
      "PROD 2024 Nov NO5: 18000 rows\n",
      "PROD 2024 Dec NO5: 18600 rows\n",
      "\n",
      "Total production rows fetched: 3,288,000\n",
      "Production rows before dedup: 3288000\n",
      "Production rows after dedup : 657600\n",
      "Final production rows: 657600\n",
      "CONS 2021 Jan NO1: 18600 rows\n",
      "CONS 2021 Feb NO1: 16800 rows\n",
      "CONS 2021 Mar NO1: 18575 rows\n",
      "CONS 2021 Apr NO1: 18000 rows\n",
      "CONS 2021 May NO1: 18600 rows\n",
      "CONS 2021 Jun NO1: 18000 rows\n",
      "CONS 2021 Jul NO1: 18600 rows\n",
      "CONS 2021 Aug NO1: 18600 rows\n",
      "CONS 2021 Sep NO1: 18000 rows\n",
      "CONS 2021 Oct NO1: 18625 rows\n",
      "CONS 2021 Nov NO1: 18000 rows\n",
      "CONS 2021 Dec NO1: 18600 rows\n",
      "CONS 2021 Jan NO2: 18600 rows\n",
      "CONS 2021 Feb NO2: 16800 rows\n",
      "CONS 2021 Mar NO2: 18575 rows\n",
      "CONS 2021 Apr NO2: 18000 rows\n",
      "CONS 2021 May NO2: 18600 rows\n",
      "CONS 2021 Jun NO2: 18000 rows\n",
      "CONS 2021 Jul NO2: 18600 rows\n",
      "CONS 2021 Aug NO2: 18600 rows\n",
      "CONS 2021 Sep NO2: 18000 rows\n",
      "CONS 2021 Oct NO2: 18625 rows\n",
      "CONS 2021 Nov NO2: 18000 rows\n",
      "CONS 2021 Dec NO2: 18600 rows\n",
      "CONS 2021 Jan NO3: 18600 rows\n",
      "CONS 2021 Feb NO3: 16800 rows\n",
      "CONS 2021 Mar NO3: 18575 rows\n",
      "CONS 2021 Apr NO3: 18000 rows\n",
      "CONS 2021 May NO3: 18600 rows\n",
      "CONS 2021 Jun NO3: 18000 rows\n",
      "CONS 2021 Jul NO3: 18600 rows\n",
      "CONS 2021 Aug NO3: 18600 rows\n",
      "CONS 2021 Sep NO3: 18000 rows\n",
      "CONS 2021 Oct NO3: 18625 rows\n",
      "CONS 2021 Nov NO3: 18000 rows\n",
      "CONS 2021 Dec NO3: 18600 rows\n",
      "CONS 2021 Jan NO4: 18600 rows\n",
      "CONS 2021 Feb NO4: 16800 rows\n",
      "CONS 2021 Mar NO4: 18575 rows\n",
      "CONS 2021 Apr NO4: 18000 rows\n",
      "CONS 2021 May NO4: 18600 rows\n",
      "CONS 2021 Jun NO4: 18000 rows\n",
      "CONS 2021 Jul NO4: 18600 rows\n",
      "CONS 2021 Aug NO4: 18600 rows\n",
      "CONS 2021 Sep NO4: 18000 rows\n",
      "CONS 2021 Oct NO4: 18625 rows\n",
      "CONS 2021 Nov NO4: 18000 rows\n",
      "CONS 2021 Dec NO4: 18600 rows\n",
      "CONS 2021 Jan NO5: 18600 rows\n",
      "CONS 2021 Feb NO5: 16800 rows\n",
      "CONS 2021 Mar NO5: 18575 rows\n",
      "CONS 2021 Apr NO5: 18000 rows\n",
      "CONS 2021 May NO5: 18600 rows\n",
      "CONS 2021 Jun NO5: 18000 rows\n",
      "CONS 2021 Jul NO5: 18600 rows\n",
      "CONS 2021 Aug NO5: 18600 rows\n",
      "CONS 2021 Sep NO5: 18000 rows\n",
      "CONS 2021 Oct NO5: 18625 rows\n",
      "CONS 2021 Nov NO5: 18000 rows\n",
      "CONS 2021 Dec NO5: 18600 rows\n",
      "CONS 2022 Jan NO1: 18600 rows\n",
      "CONS 2022 Feb NO1: 16800 rows\n",
      "CONS 2022 Mar NO1: 18575 rows\n",
      "CONS 2022 Apr NO1: 18000 rows\n",
      "CONS 2022 May NO1: 18600 rows\n",
      "CONS 2022 Jun NO1: 18000 rows\n",
      "CONS 2022 Jul NO1: 18600 rows\n",
      "CONS 2022 Aug NO1: 18600 rows\n",
      "CONS 2022 Sep NO1: 18000 rows\n",
      "CONS 2022 Oct NO1: 18625 rows\n",
      "CONS 2022 Nov NO1: 18000 rows\n",
      "CONS 2022 Dec NO1: 18600 rows\n",
      "CONS 2022 Jan NO2: 18600 rows\n",
      "CONS 2022 Feb NO2: 16800 rows\n",
      "CONS 2022 Mar NO2: 18575 rows\n",
      "CONS 2022 Apr NO2: 18000 rows\n",
      "CONS 2022 May NO2: 18600 rows\n",
      "CONS 2022 Jun NO2: 18000 rows\n",
      "CONS 2022 Jul NO2: 18600 rows\n",
      "CONS 2022 Aug NO2: 18600 rows\n",
      "CONS 2022 Sep NO2: 18000 rows\n",
      "CONS 2022 Oct NO2: 18625 rows\n",
      "CONS 2022 Nov NO2: 18000 rows\n",
      "CONS 2022 Dec NO2: 18600 rows\n",
      "CONS 2022 Jan NO3: 18600 rows\n",
      "CONS 2022 Feb NO3: 16800 rows\n",
      "CONS 2022 Mar NO3: 18575 rows\n",
      "CONS 2022 Apr NO3: 18000 rows\n",
      "CONS 2022 May NO3: 18600 rows\n",
      "CONS 2022 Jun NO3: 18000 rows\n",
      "CONS 2022 Jul NO3: 18600 rows\n",
      "CONS 2022 Aug NO3: 18600 rows\n",
      "CONS 2022 Sep NO3: 18000 rows\n",
      "CONS 2022 Oct NO3: 18625 rows\n",
      "CONS 2022 Nov NO3: 18000 rows\n",
      "CONS 2022 Dec NO3: 18600 rows\n",
      "CONS 2022 Jan NO4: 18600 rows\n",
      "CONS 2022 Feb NO4: 16800 rows\n",
      "CONS 2022 Mar NO4: 18575 rows\n",
      "CONS 2022 Apr NO4: 18000 rows\n",
      "CONS 2022 May NO4: 18600 rows\n",
      "CONS 2022 Jun NO4: 18000 rows\n",
      "CONS 2022 Jul NO4: 18600 rows\n",
      "CONS 2022 Aug NO4: 18600 rows\n",
      "CONS 2022 Sep NO4: 18000 rows\n",
      "CONS 2022 Oct NO4: 18625 rows\n",
      "CONS 2022 Nov NO4: 18000 rows\n",
      "CONS 2022 Dec NO4: 18600 rows\n",
      "CONS 2022 Jan NO5: 18600 rows\n",
      "CONS 2022 Feb NO5: 16800 rows\n",
      "CONS 2022 Mar NO5: 18575 rows\n",
      "CONS 2022 Apr NO5: 18000 rows\n",
      "CONS 2022 May NO5: 18600 rows\n",
      "CONS 2022 Jun NO5: 18000 rows\n",
      "CONS 2022 Jul NO5: 18600 rows\n",
      "CONS 2022 Aug NO5: 18600 rows\n",
      "CONS 2022 Sep NO5: 18000 rows\n",
      "CONS 2022 Oct NO5: 18625 rows\n",
      "CONS 2022 Nov NO5: 18000 rows\n",
      "CONS 2022 Dec NO5: 18600 rows\n",
      "CONS 2023 Jan NO1: 18600 rows\n",
      "CONS 2023 Feb NO1: 16800 rows\n",
      "CONS 2023 Mar NO1: 18575 rows\n",
      "CONS 2023 Apr NO1: 18000 rows\n",
      "CONS 2023 May NO1: 18600 rows\n",
      "CONS 2023 Jun NO1: 18000 rows\n",
      "CONS 2023 Jul NO1: 18600 rows\n",
      "CONS 2023 Aug NO1: 18600 rows\n",
      "CONS 2023 Sep NO1: 18000 rows\n",
      "CONS 2023 Oct NO1: 18625 rows\n",
      "CONS 2023 Nov NO1: 18000 rows\n",
      "CONS 2023 Dec NO1: 18600 rows\n",
      "CONS 2023 Jan NO2: 18600 rows\n",
      "CONS 2023 Feb NO2: 16800 rows\n",
      "CONS 2023 Mar NO2: 18575 rows\n",
      "CONS 2023 Apr NO2: 18000 rows\n",
      "CONS 2023 May NO2: 18600 rows\n",
      "CONS 2023 Jun NO2: 18000 rows\n",
      "CONS 2023 Jul NO2: 18600 rows\n",
      "CONS 2023 Aug NO2: 18600 rows\n",
      "CONS 2023 Sep NO2: 18000 rows\n",
      "CONS 2023 Oct NO2: 18625 rows\n",
      "CONS 2023 Nov NO2: 18000 rows\n",
      "CONS 2023 Dec NO2: 18600 rows\n",
      "CONS 2023 Jan NO3: 18600 rows\n",
      "CONS 2023 Feb NO3: 16800 rows\n",
      "CONS 2023 Mar NO3: 18575 rows\n",
      "CONS 2023 Apr NO3: 18000 rows\n",
      "CONS 2023 May NO3: 18600 rows\n",
      "CONS 2023 Jun NO3: 18000 rows\n",
      "CONS 2023 Jul NO3: 18600 rows\n",
      "CONS 2023 Aug NO3: 18600 rows\n",
      "CONS 2023 Sep NO3: 18000 rows\n",
      "CONS 2023 Oct NO3: 18625 rows\n",
      "CONS 2023 Nov NO3: 18000 rows\n",
      "CONS 2023 Dec NO3: 18600 rows\n",
      "CONS 2023 Jan NO4: 18600 rows\n",
      "CONS 2023 Feb NO4: 16800 rows\n",
      "CONS 2023 Mar NO4: 18575 rows\n",
      "CONS 2023 Apr NO4: 18000 rows\n",
      "CONS 2023 May NO4: 18600 rows\n",
      "CONS 2023 Jun NO4: 18000 rows\n",
      "CONS 2023 Jul NO4: 18600 rows\n",
      "CONS 2023 Aug NO4: 18600 rows\n",
      "CONS 2023 Sep NO4: 18000 rows\n",
      "CONS 2023 Oct NO4: 18625 rows\n",
      "CONS 2023 Nov NO4: 18000 rows\n",
      "CONS 2023 Dec NO4: 18600 rows\n",
      "CONS 2023 Jan NO5: 18600 rows\n",
      "CONS 2023 Feb NO5: 16800 rows\n",
      "CONS 2023 Mar NO5: 18575 rows\n",
      "CONS 2023 Apr NO5: 18000 rows\n",
      "CONS 2023 May NO5: 18600 rows\n",
      "CONS 2023 Jun NO5: 18000 rows\n",
      "CONS 2023 Jul NO5: 18600 rows\n",
      "CONS 2023 Aug NO5: 18600 rows\n",
      "CONS 2023 Sep NO5: 18000 rows\n",
      "CONS 2023 Oct NO5: 18625 rows\n",
      "CONS 2023 Nov NO5: 18000 rows\n",
      "CONS 2023 Dec NO5: 18600 rows\n",
      "CONS 2024 Jan NO1: 18600 rows\n",
      "CONS 2024 Feb NO1: 17400 rows\n",
      "CONS 2024 Mar NO1: 18575 rows\n",
      "CONS 2024 Apr NO1: 18000 rows\n",
      "CONS 2024 May NO1: 18600 rows\n",
      "CONS 2024 Jun NO1: 18000 rows\n",
      "CONS 2024 Jul NO1: 18600 rows\n",
      "CONS 2024 Aug NO1: 18600 rows\n",
      "CONS 2024 Sep NO1: 18000 rows\n",
      "CONS 2024 Oct NO1: 18625 rows\n",
      "CONS 2024 Nov NO1: 18000 rows\n",
      "CONS 2024 Dec NO1: 18600 rows\n",
      "CONS 2024 Jan NO2: 18600 rows\n",
      "CONS 2024 Feb NO2: 17400 rows\n",
      "CONS 2024 Mar NO2: 18575 rows\n",
      "CONS 2024 Apr NO2: 18000 rows\n",
      "CONS 2024 May NO2: 18600 rows\n",
      "CONS 2024 Jun NO2: 18000 rows\n",
      "CONS 2024 Jul NO2: 18600 rows\n",
      "CONS 2024 Aug NO2: 18600 rows\n",
      "CONS 2024 Sep NO2: 18000 rows\n",
      "CONS 2024 Oct NO2: 18625 rows\n",
      "CONS 2024 Nov NO2: 18000 rows\n",
      "CONS 2024 Dec NO2: 18600 rows\n",
      "CONS 2024 Jan NO3: 18600 rows\n",
      "CONS 2024 Feb NO3: 17400 rows\n",
      "CONS 2024 Mar NO3: 18575 rows\n",
      "CONS 2024 Apr NO3: 18000 rows\n",
      "CONS 2024 May NO3: 18600 rows\n",
      "CONS 2024 Jun NO3: 18000 rows\n",
      "CONS 2024 Jul NO3: 18600 rows\n",
      "CONS 2024 Aug NO3: 18600 rows\n",
      "CONS 2024 Sep NO3: 18000 rows\n",
      "CONS 2024 Oct NO3: 18625 rows\n",
      "CONS 2024 Nov NO3: 18000 rows\n",
      "CONS 2024 Dec NO3: 18600 rows\n",
      "CONS 2024 Jan NO4: 18600 rows\n",
      "CONS 2024 Feb NO4: 17400 rows\n",
      "CONS 2024 Mar NO4: 18575 rows\n",
      "CONS 2024 Apr NO4: 18000 rows\n",
      "CONS 2024 May NO4: 18600 rows\n",
      "CONS 2024 Jun NO4: 18000 rows\n",
      "CONS 2024 Jul NO4: 18600 rows\n",
      "CONS 2024 Aug NO4: 18600 rows\n",
      "CONS 2024 Sep NO4: 18000 rows\n",
      "CONS 2024 Oct NO4: 18625 rows\n",
      "CONS 2024 Nov NO4: 18000 rows\n",
      "CONS 2024 Dec NO4: 18600 rows\n",
      "CONS 2024 Jan NO5: 18600 rows\n",
      "CONS 2024 Feb NO5: 17400 rows\n",
      "CONS 2024 Mar NO5: 18575 rows\n",
      "CONS 2024 Apr NO5: 18000 rows\n",
      "CONS 2024 May NO5: 18600 rows\n",
      "CONS 2024 Jun NO5: 18000 rows\n",
      "CONS 2024 Jul NO5: 18600 rows\n",
      "CONS 2024 Aug NO5: 18600 rows\n",
      "CONS 2024 Sep NO5: 18000 rows\n",
      "CONS 2024 Oct NO5: 18625 rows\n",
      "CONS 2024 Nov NO5: 18000 rows\n",
      "CONS 2024 Dec NO5: 18600 rows\n",
      "\n",
      "Total consumption rows fetched: 4,383,000\n",
      "Consumption rows before dedup: 4383000\n",
      "Consumption rows after dedup : 876600\n",
      "Final consumption rows: 876600\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pricearea</th>\n",
       "      <th>consumptiongroup</th>\n",
       "      <th>starttime</th>\n",
       "      <th>quantitykwh</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NO1</td>\n",
       "      <td>cabin</td>\n",
       "      <td>2020-12-31 23:00:00+00:00</td>\n",
       "      <td>177071.56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NO1</td>\n",
       "      <td>cabin</td>\n",
       "      <td>2021-01-01 00:00:00+00:00</td>\n",
       "      <td>171335.12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NO1</td>\n",
       "      <td>cabin</td>\n",
       "      <td>2021-01-01 01:00:00+00:00</td>\n",
       "      <td>164912.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NO1</td>\n",
       "      <td>cabin</td>\n",
       "      <td>2021-01-01 02:00:00+00:00</td>\n",
       "      <td>160265.77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NO1</td>\n",
       "      <td>cabin</td>\n",
       "      <td>2021-01-01 03:00:00+00:00</td>\n",
       "      <td>159828.69</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  pricearea consumptiongroup                 starttime  quantitykwh\n",
       "0       NO1            cabin 2020-12-31 23:00:00+00:00    177071.56\n",
       "1       NO1            cabin 2021-01-01 00:00:00+00:00    171335.12\n",
       "2       NO1            cabin 2021-01-01 01:00:00+00:00    164912.02\n",
       "3       NO1            cabin 2021-01-01 02:00:00+00:00    160265.77\n",
       "4       NO1            cabin 2021-01-01 03:00:00+00:00    159828.69"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from dateutil.relativedelta import relativedelta\n",
    "\n",
    "# ----------------- Configuration -----------------\n",
    "BASE_URL = \"https://api.elhub.no/energy-data/v0/price-areas\"\n",
    "TIMEZONE_OFFSET = \"%2B02:00\"\n",
    "PRICE_AREAS = [\"NO1\", \"NO2\", \"NO3\", \"NO4\", \"NO5\"]\n",
    "\n",
    "# Production configuration\n",
    "DATASET_PROD = \"PRODUCTION_PER_GROUP_MBA_HOUR\"\n",
    "YEARS_PROD = [2022, 2023, 2024]\n",
    "\n",
    "# Consumption configuration\n",
    "DATASET_CONS = \"CONSUMPTION_PER_GROUP_MBA_HOUR\"\n",
    "YEARS_CONS = [2021, 2022, 2023, 2024]\n",
    "\n",
    "\n",
    "def month_edges(year: int):\n",
    "    \"\"\"Generate month start/end pairs for a given year.\"\"\"\n",
    "    cur = datetime(year, 1, 1)\n",
    "    end = datetime(year + 1, 1, 1)\n",
    "    while cur < end:\n",
    "        nxt = cur + relativedelta(months=1)\n",
    "        yield cur, min(nxt, end)\n",
    "        cur = nxt\n",
    "\n",
    "\n",
    "def fetch_month(start_dt: datetime, end_dt: datetime, area: str, dataset: str, top_key: str):\n",
    "    \"\"\"Download one month of data for one price area for the given dataset.\"\"\"\n",
    "    start_str = start_dt.strftime(\"%Y-%m-%dT%H:%M:%S\") + TIMEZONE_OFFSET\n",
    "    end_str = end_dt.strftime(\"%Y-%m-%dT%H:%M:%S\") + TIMEZONE_OFFSET\n",
    "\n",
    "    url = (\n",
    "        f\"{BASE_URL}?dataset={dataset}\"\n",
    "        f\"&startDate={start_str}&endDate={end_str}\"\n",
    "        f\"&priceArea={area}\"\n",
    "    )\n",
    "\n",
    "    r = requests.get(url, timeout=60)\n",
    "    r.raise_for_status()\n",
    "    js = r.json()\n",
    "\n",
    "    # Case 1: data appears directly as a top-level list\n",
    "    if isinstance(js.get(top_key), list):\n",
    "        return js[top_key]\n",
    "\n",
    "    # Case 2: data inside \"data\" → \"attributes\"\n",
    "    rows = []\n",
    "    for entity in js.get(\"data\", []):\n",
    "        attrs = (entity or {}).get(\"attributes\", {}) or {}\n",
    "        rows.extend(attrs.get(top_key, []))\n",
    "    return rows\n",
    "\n",
    "\n",
    "# ----------------- Production 2022–2024 -----------------\n",
    "prod_rows = []\n",
    "\n",
    "for year in YEARS_PROD:\n",
    "    for area in PRICE_AREAS:\n",
    "        for s, e in month_edges(year):\n",
    "            try:\n",
    "                rows = fetch_month(s, e, area, DATASET_PROD, \"productionPerGroupMbaHour\")\n",
    "                print(f\"PROD {year} {s:%b} {area}: {len(rows)} rows\")\n",
    "                prod_rows.extend(rows)\n",
    "            except Exception as ex:\n",
    "                print(f\"ERROR PROD {year} {s:%b} {area}: {ex}\")\n",
    "\n",
    "print(f\"\\nTotal production rows fetched: {len(prod_rows):,}\")\n",
    "\n",
    "raw_prod = pd.DataFrame(prod_rows)\n",
    "\n",
    "expected_prod_cols = {\n",
    "    \"priceArea\": \"pricearea\",\n",
    "    \"productionGroup\": \"productiongroup\",\n",
    "    \"startTime\": \"starttime\",\n",
    "    \"quantityKwh\": \"quantitykwh\",\n",
    "}\n",
    "\n",
    "missing = [c for c in expected_prod_cols if c not in raw_prod.columns]\n",
    "if missing:\n",
    "    raise ValueError(f\"Missing columns in production data: {missing}\")\n",
    "\n",
    "df_prod = (\n",
    "    raw_prod[list(expected_prod_cols)]\n",
    "    .rename(columns=expected_prod_cols)\n",
    "    .copy()\n",
    ")\n",
    "\n",
    "df_prod[\"starttime\"] = pd.to_datetime(df_prod[\"starttime\"], errors=\"coerce\", utc=True)\n",
    "df_prod[\"quantitykwh\"] = pd.to_numeric(df_prod[\"quantitykwh\"], errors=\"coerce\")\n",
    "\n",
    "df_prod = df_prod.dropna(subset=[\"pricearea\", \"productiongroup\", \"starttime\", \"quantitykwh\"])\n",
    "\n",
    "before = len(df_prod)\n",
    "df_prod_norm = df_prod.drop_duplicates(subset=[\"pricearea\", \"productiongroup\", \"starttime\"])\n",
    "after = len(df_prod_norm)\n",
    "\n",
    "print(\"Production rows before dedup:\", before)\n",
    "print(\"Production rows after dedup :\", after)\n",
    "print(\"Final production rows:\", len(df_prod_norm))\n",
    "\n",
    "\n",
    "# ----------------- Consumption 2021–2024 -----------------\n",
    "cons_rows = []\n",
    "\n",
    "for year in YEARS_CONS:\n",
    "    for area in PRICE_AREAS:\n",
    "        for s, e in month_edges(year):\n",
    "            try:\n",
    "                rows = fetch_month(s, e, area, DATASET_CONS, \"consumptionPerGroupMbaHour\")\n",
    "                print(f\"CONS {year} {s:%b} {area}: {len(rows)} rows\")\n",
    "                cons_rows.extend(rows)\n",
    "            except Exception as ex:\n",
    "                print(f\"ERROR CONS {year} {s:%b} {area}: {ex}\")\n",
    "\n",
    "print(f\"\\nTotal consumption rows fetched: {len(cons_rows):,}\")\n",
    "\n",
    "raw_cons = pd.DataFrame(cons_rows)\n",
    "\n",
    "expected_cons_cols = {\n",
    "    \"priceArea\": \"pricearea\",\n",
    "    \"consumptionGroup\": \"consumptiongroup\",\n",
    "    \"startTime\": \"starttime\",\n",
    "    \"quantityKwh\": \"quantitykwh\",\n",
    "}\n",
    "\n",
    "missing = [c for c in expected_cons_cols if c not in raw_cons.columns]\n",
    "if missing:\n",
    "    raise ValueError(f\"Missing columns in consumption data: {missing}\")\n",
    "\n",
    "df_cons = (\n",
    "    raw_cons[list(expected_cons_cols)]\n",
    "    .rename(columns=expected_cons_cols)\n",
    "    .copy()\n",
    ")\n",
    "\n",
    "df_cons[\"starttime\"] = pd.to_datetime(df_cons[\"starttime\"], errors=\"coerce\", utc=True)\n",
    "df_cons[\"quantitykwh\"] = pd.to_numeric(df_cons[\"quantitykwh\"], errors=\"coerce\")\n",
    "\n",
    "df_cons = df_cons.dropna(subset=[\"pricearea\", \"consumptiongroup\", \"starttime\", \"quantitykwh\"])\n",
    "\n",
    "before = len(df_cons)\n",
    "df_cons_norm = df_cons.drop_duplicates(subset=[\"pricearea\", \"consumptiongroup\", \"starttime\"])\n",
    "after = len(df_cons_norm)\n",
    "\n",
    "print(\"Consumption rows before dedup:\", before)\n",
    "print(\"Consumption rows after dedup :\", after)\n",
    "print(\"Final consumption rows:\", len(df_cons_norm))\n",
    "\n",
    "df_cons_norm.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f148d0a0",
   "metadata": {},
   "source": [
    "## Interpretation of fetched consumption data\n",
    "\n",
    "The Elhub API returned a total of 4.38 million raw consumption rows for the\n",
    "period 2021–2024 across all price areas. After cleaning (removing invalid rows)\n",
    "and dropping duplicates on the combination of (pricearea, consumptiongroup,\n",
    "starttime), the dataset was reduced to 876,600 unique hourly records.\n",
    "\n",
    "This cleaned dataset will now be written into Cassandra using Spark in the next step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "65d816dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote to Cassandra: my_keyspace.production_per_group_hour_2022_2024\n"
     ]
    }
   ],
   "source": [
    "# --- Write production 2022–2024 to Cassandra ---\n",
    "\n",
    "# We reuse KEYSPACE, PROD_TABLE and spark_sess from earlier cells\n",
    "\n",
    "# Convert Pandas → Spark\n",
    "sdf_prod = spark_sess.createDataFrame(df_prod_2022_2024)\n",
    "\n",
    "# Partition by area and group for better distribution\n",
    "sdf_prod = sdf_prod.repartition(\"pricearea\", \"productiongroup\")\n",
    "\n",
    "# Write to Cassandra\n",
    "(\n",
    "    sdf_prod.write\n",
    "    .format(\"org.apache.spark.sql.cassandra\")\n",
    "    .options(keyspace=KEYSPACE, table=PROD_TABLE)\n",
    "    .mode(\"append\")\n",
    "    .save()\n",
    ")\n",
    "\n",
    "print(f\"Wrote to Cassandra: {KEYSPACE}.{PROD_TABLE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6168acee",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "274865ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows in production_per_group_hour_2022_2024: 657,525\n",
      "Distinct price areas: 5 | Distinct production groups: 5\n",
      "+---------+---------------+-------------------+-----------+\n",
      "|pricearea|productiongroup|          starttime|quantitykwh|\n",
      "+---------+---------------+-------------------+-----------+\n",
      "|      NO2|           wind|2022-01-01 00:00:00|   309870.0|\n",
      "|      NO1|        thermal|2022-01-01 00:00:00|   30049.92|\n",
      "|      NO5|          solar|2022-01-01 00:00:00|     42.136|\n",
      "|      NO1|          hydro|2022-01-01 00:00:00|  1291422.4|\n",
      "|      NO4|          hydro|2022-01-01 00:00:00|  3840983.2|\n",
      "+---------+---------------+-------------------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# --- Verify production table in Cassandra ---\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "sdf_prod_cas = (\n",
    "    spark_sess.read\n",
    "    .format(\"org.apache.spark.sql.cassandra\")\n",
    "    .options(keyspace=KEYSPACE, table=PROD_TABLE)\n",
    "    .load()\n",
    "    .select(\"pricearea\", \"productiongroup\", \"starttime\", \"quantitykwh\")\n",
    ")\n",
    "\n",
    "rows_prod = sdf_prod_cas.count()\n",
    "areas_prod = sdf_prod_cas.select(\"pricearea\").distinct().count()\n",
    "groups_prod = sdf_prod_cas.select(\"productiongroup\").distinct().count()\n",
    "\n",
    "print(f\"Rows in {PROD_TABLE}: {rows_prod:,}\")\n",
    "print(f\"Distinct price areas: {areas_prod} | Distinct production groups: {groups_prod}\")\n",
    "\n",
    "# Quick preview\n",
    "sdf_prod_cas.orderBy(\"starttime\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dcffd47",
   "metadata": {},
   "source": [
    "## Verifying the Cassandra production table\n",
    "\n",
    "After writing the cleaned production dataset to Cassandra, I verify that the\n",
    "table was stored correctly by reading it back with Spark. Here I check:\n",
    "\n",
    "- total number of rows written  \n",
    "- number of distinct price areas  \n",
    "- number of distinct production groups  \n",
    "- and preview a few rows ordered by time\n",
    "\n",
    "This confirms that the Cassandra write operation worked as expected.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c9d5b6d",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "81574d84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Spark DataFrame from df_cons_norm ...\n",
      "Writing consumption data to Cassandra table: consumption_per_group_hour_2021_2024\n",
      "Write complete.\n"
     ]
    }
   ],
   "source": [
    "# Write normalized consumption data (2021–2024) to Cassandra\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "print(\"Creating Spark DataFrame from df_cons_norm ...\")\n",
    "sdf_cons = spark_sess.createDataFrame(df_cons_norm)\n",
    "\n",
    "# Færre, men fornuftige partisjoner\n",
    "spark_sess.conf.set(\"spark.sql.shuffle.partitions\", \"64\")\n",
    "\n",
    "# Repartition to match primary key (pricearea, consumptiongroup)\n",
    "sdf_cons = sdf_cons.repartition(\"pricearea\", \"consumptiongroup\")\n",
    "\n",
    "print(\"Writing consumption data to Cassandra table:\", CONS_TABLE)\n",
    "\n",
    "(\n",
    "    sdf_cons.write\n",
    "    .format(\"org.apache.spark.sql.cassandra\")\n",
    "    .options(keyspace=KEYSPACE, table=CONS_TABLE)\n",
    "    .mode(\"append\")\n",
    "    .save()\n",
    ")\n",
    "\n",
    "print(\"Write complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "879de09c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "39b74d8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows in consumption_per_group_hour_2021_2024: 876,500\n",
      "Distinct price areas: 5 | Distinct consumption groups: 5\n",
      "+---------+----------------+-------------------+-----------+\n",
      "|pricearea|consumptiongroup|          starttime|quantitykwh|\n",
      "+---------+----------------+-------------------+-----------+\n",
      "|      NO2|        tertiary|2021-01-01 00:00:00|  608354.44|\n",
      "|      NO2|        tertiary|2021-01-01 01:00:00|   606207.2|\n",
      "|      NO2|        tertiary|2021-01-01 02:00:00|  603700.75|\n",
      "|      NO2|        tertiary|2021-01-01 03:00:00|  606288.25|\n",
      "|      NO2|        tertiary|2021-01-01 04:00:00|   615663.1|\n",
      "+---------+----------------+-------------------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Verify consumption data in Cassandra\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "sdf_cons_cas = (\n",
    "    spark_sess.read\n",
    "    .format(\"org.apache.spark.sql.cassandra\")\n",
    "    .options(keyspace=KEYSPACE, table=CONS_TABLE)\n",
    "    .load()\n",
    ")\n",
    "\n",
    "rows = sdf_cons_cas.count()\n",
    "areas = sdf_cons_cas.select(\"pricearea\").distinct().count()\n",
    "groups = sdf_cons_cas.select(\"consumptiongroup\").distinct().count()\n",
    "\n",
    "print(f\"Rows in {CONS_TABLE}: {rows:,}\")\n",
    "print(f\"Distinct price areas: {areas} | Distinct consumption groups: {groups}\")\n",
    "sdf_cons_cas.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c59a8556",
   "metadata": {},
   "source": [
    "The verification confirms that the consumption table was successfully written to\n",
    "Cassandra. The row count matches the number of cleaned records inserted, and all\n",
    "price areas and consumption groups are present. The preview also shows that the\n",
    "timestamps and kWh values were stored correctly. This completes the ingestion\n",
    "pipeline for both production and consumption data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "895addd3",
   "metadata": {},
   "source": [
    "## Writing production and consumption data to MongoDB\n",
    "\n",
    "In this step I connect to MongoDB Atlas using the URI stored in the `.env` file.\n",
    "I create two new collections for Part 4:\n",
    "- `elhub_production_data_2022_2024`\n",
    "- `elhub_consumption_data_2021_2024`\n",
    "\n",
    "I then insert the cleaned datasets `df_prod_norm` and `df_cons_norm` directly\n",
    "into these collections. Before inserting, I optionally clear old documents to\n",
    "ensure a fresh dataset. The final counts in the output confirm that all rows\n",
    "were inserted correctly.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "604bcef7",
   "metadata": {},
   "source": [
    "MongoDB - production 2022-2024 + consumption 2021-2024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3777ab6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MongoDB connection OK\n",
      "Using collections: elhub_production_data_2022_2024 and elhub_consumption_data_2021_2024\n"
     ]
    }
   ],
   "source": [
    "# --- MongoDB connection setup ---\n",
    "from pymongo.mongo_client import MongoClient\n",
    "from pymongo.server_api import ServerApi\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "load_dotenv()  # reads .env locally (same as in app.py)\n",
    "uri = os.getenv(\"MONGODB_URI\")\n",
    "if not uri:\n",
    "   raise RuntimeError(\"MONGODB_URI is missing in .env\")\n",
    "# Connect to MongoDB Atlas\n",
    "client = MongoClient(uri, server_api=ServerApi('1'))\n",
    "client.admin.command(\"ping\")\n",
    "print(\"MongoDB connection OK\")\n",
    "db = client[\"ind320\"]\n",
    "# New collections for part 4\n",
    "prod_coll = db[\"elhub_production_data_2022_2024\"]\n",
    "cons_coll = db[\"elhub_consumption_data_2021_2024\"]\n",
    "print(\"Using collections:\", prod_coll.name, \"and\", cons_coll.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cee2771",
   "metadata": {},
   "source": [
    "Save df_prod_norm and df_cons_norm in MongoDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b50ee30f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserting production data into MongoDB...\n",
      "Production data inserted: 657600\n",
      "Inserting consumption data into MongoDB...\n",
      "Consumption data inserted: 876600\n"
     ]
    }
   ],
   "source": [
    "# --- Insert Production Data (2022–2024) into MongoDB ---\n",
    "print(\"Inserting production data into MongoDB...\")\n",
    "prod_coll.delete_many({})      # optional: clean old data\n",
    "prod_coll.insert_many(df_prod_norm.to_dict(\"records\"))\n",
    "print(\"Production data inserted:\", prod_coll.count_documents({}))\n",
    "# --- Insert Consumption Data (2021–2024) into MongoDB ---\n",
    "print(\"Inserting consumption data into MongoDB...\")\n",
    "cons_coll.delete_many({})      # optional\n",
    "cons_coll.insert_many(df_cons_norm.to_dict(\"records\"))\n",
    "print(\"Consumption data inserted:\", cons_coll.count_documents({}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54effe0b",
   "metadata": {},
   "source": [
    "The output confirms that both datasets were successfully inserted into MongoDB.\n",
    "The production collection contains 657,600 documents, and the consumption\n",
    "collection contains 876,600 documents, matching the cleaned DataFrame sizes.\n",
    "This completes the MongoDB storage step for Part 4.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bc1bd43",
   "metadata": {},
   "source": [
    "Sliding Window Correlation – Summary\n",
    "\n",
    "I implemented the sliding-window correlation in Streamlit with Plotly and tested it using different meteorological variables, energy types, lags, and window lengths.\n",
    "\n",
    "Findings\n",
    "\n",
    "Correlation changes noticeably through the year and is not constant.\n",
    "\n",
    "During stable weather, the correlation appears smoother and more predictable.\n",
    "\n",
    "Extreme events (temperature drops, storms, heavy precipitation) cause clear spikes or dips, especially with shorter windows.\n",
    "\n",
    "Longer windows smooth the curve, while lag adjustments reveal delayed energy responses.\n",
    "\n",
    "The interactive tool made these variations easy to observe and helped link specific weather conditions to changes in energy production and consumption."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d87d0ed5",
   "metadata": {},
   "source": [
    "## Final Summary\n",
    "\n",
    "In Part 4, I completed the workflow for downloading, cleaning, and storing the full Elhub datasets (2021–2024) and loading them into both Cassandra and MongoDB. After verifying the schemas, the data were successfully written and checked using Spark.\n",
    "\n",
    "One issue I encountered was that old tables from a previous project were still present. These duplicates caused the new Elhub data to appear incomplete. After removing the outdated tables and recreating the correct ones, the imports worked properly and the row counts matched the API output.\n",
    "\n",
    "With the corrected data pipeline, all Streamlit components—STL decomposition, spectrograms, sliding-window correlation, anomaly detection, and forecasting—now work as intended using the updated datasets. This completes the end-to-end setup for the IND320 project and ensures that all analyses run on clean and consistent data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "214859d7",
   "metadata": {},
   "source": [
    "## Project Links\n",
    "\n",
    "Below are the required links to the public GitHub repository and the deployed Streamlit application:\n",
    "\n",
    "- **Streamlit App (Online Dashboard)**  \n",
    "  https://ind320-dashboard33.streamlit.app/\n",
    "\n",
    "- **GitHub Repository (Source Code & Notebooks)**  \n",
    "  https://github.com/AbdirahmanOsma/IND320\n",
    "\n",
    "These links provide access to the full data pipeline, final dashboard, and all code used throughout the project."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ind320",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
